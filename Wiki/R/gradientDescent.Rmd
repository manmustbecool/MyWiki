---
title: "GradientDescent"
output: html_document
date: "Updated: `r format(Sys.time(), '%d %B, %Y')`"
---


```{r}
# load the data
data <- read.table("gradientDescent\\ex2data1.txt", header=F, sep=",", quote ="\"'")
colnames(data) = c("exam1","exam2","admit")
  
# plot the data
with(data,plot(exam1[admit==1],exam2[admit==1]))
with(data,points(exam1[admit==0],exam2[admit==0],col="red"))

```

## logistic regression 


### logistic/sigmoid function

$$
g(z)=\frac{1}{1+e^{-z}}
$$

```{r}
sigmoid = function(z){
  1 / (1 + exp(-z))
}

```

### classification hypothesis function

$$
h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}
$$
Note from the Linear regression : 
$$
y = \theta_0+\theta_1x_1+...+\theta_nx_n =\sum_{i=1}^{n}\theta_ix_i = \theta^{T}x
$$


```{r}
hypothesis = function(x, theta){
  sigmoid(x %*% theta)
}

```



### cost function 

$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}cost(h_\theta(x),y)
$$
$$
cost(h_\theta(x),y)=-y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))
$$
$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}[-y^i\log(h_\theta(x^i)) - (1-y^i)\log(1- h_\theta(x^i))]
$$

```{r}
cost = function(x, y, theta){
  hx = hypothesis(x, theta)
  m = nrow(x)
  # (1/m) * (((-t(y) %*% log(hx)) - t(1-y) %*% log(1 - hx)))
  (1/m) * sum( -y*log(hx) - (1-y)*log(1-hx) )
}

```

### (batch) gradient descent

minimize $J(\theta)$

$$
\theta := \theta - \alpha \sum_{i=1}^{n}(y_i - h_\theta(x_i))x_i
$$

```{r}

gradient <- function(x, y, theta) {
  hx = hypothesis(x, theta)
  m = nrow(x)
  # (1/m) * ( t(hx - y) %*% x )
  (1/m) * (t(x) %*% (hx - y))
}

```


```{r}

alpha = 0.001
max_interation = 600

# initialize coefficients
#-1.3,0.01,0.01
theta = matrix(c(0,0,0),nrow=3)


m = nrow(data)
y = as.matrix(data[,3])
x = as.matrix(data[,c(1,2)])
x = cbind(1,x)


# keep history
cost_history <- double(max_interation)
theta_history <- list(max_interation)


for (i in 1:max_interation){
  theta = theta - alpha * gradient(x, y, theta)
  
  cost_history[i] <- cost(x, y, theta) # for plot
  theta_history[[i]] <- theta
  
}

# -------- compare with lm function implementation--------------
print(theta)

res <- lm(admit ~ exam1 + exam2, data=data)
print(res)

```

```{r}
plot(cost_history)
# plot(theta_history)


#---------- test accuracy -----------------------------------
m <- nrow(data)
test_data <- matrix(c(rep(1, m), data$exam1, data$exam2), nrow = m)

data$predict <- sigmoid(test_data %*% theta)

accuracy <- with(data, mean(ifelse(round(predict) == admit, 1, 0)))
accuracy

```

https://dernk.wordpress.com/2013/06/


## Other reference

Linear regression by gradient descents

http://www.r-bloggers.com/linear-regression-by-gradient-descent/?goback=%2Egmp_70219%2Egde_70219_member_139405437


