---
title: "R Machine Learning"
output: html_document
---

## Gradient descents 

Linear regression by gradient descents

http://www.r-bloggers.com/linear-regression-by-gradient-descent/?goback=%2Egmp_70219%2Egde_70219_member_139405437

## Neural networks ##

http://www.ai-junkie.com/ann/evolved/nnt8.html


## Hidden Markov Models ##

http://web.stanford.edu/class/stats366/hmmR2.html

http://yunus.hacettepe.edu.tr/~iozkan/eco742/hmm.html


## Logistic Regression ##

https://rdatasmith.wordpress.com/2012/05/16/programming-problem-set-2-part-1-logistic-regression/


## Support Vector Machine (SVM) ##

  *  **e1071**

tune
http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=e1071:tune

svm
http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=e1071:svm

http://stackoverflow.com/questions/7390173/svm-equations-from-e1071-r-package

Recursive Feature Extraction (SVM-RFE)

http://stats.stackexchange.com/questions/10676/using-an-svm-for-feature-selection



## Text mining

http://chengjun.github.io/en/2014/04/sentiment-analysis-with-machine-learning-in-R/


```r
rdmTweets <- list(
"Text Mining Tutorial http://t.co/jPHHLEGm",
"He likes dogs r Singapore http://t.co/GPA0TyG5",
"RDataMining: Easier Parallel Computing in R with snowfall and sfCluster http://t.co/BPcinvzK",
"RDataMining: Tutorial: Parallel computing using R package snowfall http://t.co/CHBCyr76",
"handling big data: Interacting with Data using the filehash Package for R http://t.co/7RB3sChx"
)

df <- as.data.frame(do.call(rbind, rdmTweets))
names(df) <- c('text')

library(tm)
# ---- build a corpus
myCorpus <- Corpus(VectorSource(df$text)) # VectorSource specifies the text source

# ---- Transforming Text

myCorpus <- tm_map(myCorpus, content_transformer(tolower)) #  to lower case
myCorpus <- tm_map(myCorpus, removePunctuation) # remove punctuation
myCorpus <- tm_map(myCorpus, removeNumbers) # remove numbers
myStopwords <- c(stopwords('english'), "available", "dogs") # add two additional stopwords
myStopwords <- setdiff(myStopwords, c("r", "big")) # remove 'r' and 'big' from stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords) # remove stopwords

# ---- Stemming Words

dictCorpus <- myCorpus # keep a copy as a dictionary for stem completion
#library("SnowballC") # for stemDocument
myCorpus <- tm_map(myCorpus, stemDocument) # stem words
inspect(myCorpus)
# myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus) # stem completion
# the following stem completion works in tm v0.6 
tm_map(myCorpus, content_transformer(function(x, d)
  paste(stemCompletion(strsplit(stemDocument(x), ' ')[[1]], d), collapse = ' ')), dictCorpus)
inspect(myCorpus)
inspect(dictCorpus)

# ---- Building a Document-Term Matrix

myDtm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
inspect(myDtm)
#                  Docs
# Terms             1 2 3 4 5
#   big             0 0 0 0 1
#   comput          0 0 1 1 0
#   data            0 0 0 0 2  
#   ...

# Based on the above matrix, many data mining tasks can be done, for example, clustering, classification and association analysis.

# ----- Frequent Terms and Associations

findFreqTerms(myDtm, lowfreq=2)

# which words are associated with "r"?
findAssocs(myDtm, 'r', 0.30)
```


## Decision Tree

### Random forest tree

 * randomForest

```r
library(randomForest)

## Classification:
##data(iris)

iris[sample(nrow(iris), 5), ]
#         Sepal.Length Sepal.Width Petal.Length Petal.Width Species
# 49           5.3         3.7          1.5         0.2     setosa
# 133          6.4         2.8          5.6         2.2  virginica
# 1            5.1         3.5          1.4         0.2     setosa
# 99           5.1         2.5          3.0         1.1 versicolor
# 69           6.2         2.2          4.5         1.5 versicolor


#----------------------------------------------------------

set.seed(1)
iris.rf <- randomForest(Species ~ ., data=iris, proximity=TRUE, keep.forest=TRUE)

# ntree: Number of trees to grow. 
# importance: Should importance of predictors be assessed?
# sampsize : for example, sampsize=c(20, 30, 20), stratified sampling: draw 20, 30, and 20 of the species to grow each tree.


## --------- Plot of feature important 

imp <- importance(iris.rf) # imp is a matrix
plot(imp)
lines(imp)
text(imp, labels=rownames(imp), cex=1)


## --------- more detailed variable importance information

iris.rf <- randomForest(Species ~ ., data=iris, importance=TRUE, proximity=TRUE)
print(iris.rf)

# Look at variable importance:
round(importance(iris.rf), 2)


## --------- Multi-dimensional Scaling Plot of Proximity matrix from randomForest  -------------

## The unsupervised clustering case:

# MDSplot(rf, fac, ...)
# rf= randomForest object. fac=a factor that was used as response to train rf.
x<-MDSplot(iris.rf, iris$Species)

#add legend
legend("topleft", legend=levels(iris.rf$predicted), 
       fill=brewer.pal(length(levels(iris.rf$predicted)), "Set1"))


str(x)

# need to identify points?
# text(x$points, labels=attr(x$points,"dimnames")[[1]], cex=0.5)


##--------------------   ------------

## Do MDS on 1 - proximity:
iris.mds <- cmdscale(1 - iris.rf$proximity, eig=TRUE)


plot(iris.mds$points[,1], iris.mds$points[,2], type = "n", xlab = "", ylab = "", axes = FALSE,
     main = "cmdscale (stats)")



op <- par(pty="s")
x <- cbind(iris[,1:4], iris.mds$points)
pairs(x, cex=0.6, gap=0,
      col=c("red", "green", "blue")[as.numeric(iris$Species)],
      main="Iris Data: Predictors and MDS of Proximity Based on RandomForest")
par(op)


#  "GOF" component of the result ("goodness of fit")
print(iris.mds$GOF)

```

###Recursive Partitioning and Regression Trees

 * rpart
 
http://www.statmethods.net/advstats/cart.html

```r
# Classification Tree with rpart
library(rpart)

kyphosis[1:3,]
#   Kyphosis Age Number Start
# 1   absent  71      3     5
# 2   absent 158      3    14
# 3  present 128      4     5

# grow tree  
fit <- rpart(Kyphosis ~ Age + Number + Start, method="class", data=kyphosis) # classfication tree

printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits

# plot tree
plot(fit, uniform=TRUE, main="Classification Tree for Kyphosis")
text(fit, use.n=TRUE, all=TRUE, cex=.8)

# create attractive postscript plot of tree
post(fit, file = "",
     title = "Classification Tree for Kyphosis")

# prune the tree
pfit<- prune(fit, cp=fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])

# plot the pruned tree
plot(pfit, uniform=TRUE,
     main="Pruned Classification Tree for Kyphosis")
text(pfit, use.n=TRUE, all=TRUE, cex=.8)
post(pfit,  file = "",
     title = "Pruned Classification Tree for Kyphosis")
```

## Association rule learning ##

 * arulesSequences

```r
# sample events 
# 322282,20100827,2,AA,BB
# 312980,20100622,3,CC,DD,EE
# 312246,20100218,7,FF,GG,HH,II,MM,OO,NN

t <- read_baskets(con=system.file("misc", "zaki.txt", package="arulesSequences"), 
         info=c("eventID","sequenceID","size"))

# get all eventId
transactionInfo(t)$eventID

# mine frequent sequences
s1 <- cspade(t, parameter=list(support = 0.4), 
           control=list(verbose = TRUE))
summary(s1)
as(s1, "data.frame")
```

## Recommender system ##

  * recommenderlab

## Feature selection ##

  * caret

http://www.cybaea.net/Blogs/Data/Feature-selection-Using-the-caret-package.html


## Dimensionality Reduction

### Singular Value Decomposition (SVD) ##

### Multidimensional Scaling (MDS) ##

```r
#eurodist (dist object) that gives the road distances (in km) between 21 cities in Europe. 
euromat = as.matrix(eurodist) # convert eurodist to matrix
euromat[1:5, 1:5] # inspect first five elements

as.dist(myMatrix) # convert to dist object from co matrix


# MDS 'cmdscale'
mds1 = cmdscale(eurodist, k = 2)

# plot
plot(mds1[,1], mds1[,2], type = "n", xlab = "", ylab = "", axes = FALSE,
     main = "cmdscale (stats)")
text(mds1[,1], mds1[,2], labels(eurodist), cex = 0.9, xpd = TRUE)

#------------------ 

data(iris)

# calculate the distance matrix for first 4 columns
irisDist <- dist(iris[,1:4])

irisMds = cmdscale(irisDist, k = 2)

# plot with color based on the 5th column 
plot(irisMds)
text(irisMds[,1], (irisMds[,2]+max(irisMds[,2])*0.05), labels=1:nrow(irisMds), cex = 0.6, xpd = TRUE)
points(irisMds, col=c("red", "green", "blue")[as.numeric(iris$Species)],)

```